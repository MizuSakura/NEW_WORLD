import torch
import torch.nn as nn
import joblib
from pathlib import Path
import pandas as pd
import time 
import shutil
import tempfile

# ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç path ‡πÅ‡∏•‡∏∞ import ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
from src.models.lstm_model import LSTMModel
from src.data.generator import data_generator
from src.data.format_duration_time import format_duration

SCALER_REFERANCE_FOLDER = r"D:\Project_end\mainproject_fix\main_project\config\predict_model"
SCALER_FINE_TUNE_FOLDER = r"D:\Project_end\mainproject_fix\main_project\config\fine_tune"
MODEL_FOLDER = r"D:\Project_end\mainproject_fix\main_project\experiments\model_ex01"
NEW_DATA_FOLDER = r"D:\Project_end\mainproject_fix\main_project\data\raw" 
MODEL_FINE_TUNE_FOLDER = r"D:\Project_end\mainproject_fix\main_project\experiments\fine_tune_model"
# Hyperparameters ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fine-Tuning
WINDOW_SIZE = 30
BATCH_SIZE = 64
HIDDEN_SIZE = 50
NUM_LAYERS = 1
LEARNING_RATE = 0.001
NUM_EPOCHS = 10
   
def fine_tune_scale():
    print("---  Start Fine-Tuning ---")
    
    try:
        input_scaler_path = Path(SCALER_REFERANCE_FOLDER) / "scaler_input.pkl"
        output_scaler_path = Path(SCALER_REFERANCE_FOLDER) / "scaler_output.pkl"
        scaler_input = joblib.load(input_scaler_path)
        scaler_output = joblib.load(output_scaler_path)

        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà
        new_files = list(Path(NEW_DATA_FOLDER).glob("*.csv"))
        if not new_files:
            print("‚ö†Ô∏è File data not found Error")
            return

        # partial_fit scaler ‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà
        for file_path in new_files:
            with pd.read_csv(file_path, chunksize=10000) as reader:
                for chunk in reader:
                    scaler_input.partial_fit(chunk[['DATA_INPUT']].values)
                    scaler_output.partial_fit(chunk[['DATA_OUTPUT']].values)

        # ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå zip ‡∏ï‡∏≤‡∏°‡∏ß‡∏±‡∏ô‡πÄ‡∏ß‡∏•‡∏≤
        timestamp = time.strftime("%d_%m_%Y")
        zip_filename = Path(SCALER_FINE_TUNE_FOLDER) / f"scalers_{timestamp}.zip"

        # ‡πÉ‡∏ä‡πâ temp dir ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ü‡∏•‡πå pkl ‡∏Å‡πà‡∏≠‡∏ô zip
        with tempfile.TemporaryDirectory() as tmpdir:
            tmpdir_path = Path(tmpdir)
            input_scaler_file = tmpdir_path / f"scaler_input_{timestamp}.pkl"
            output_scaler_file = tmpdir_path / f"scaler_output_{timestamp}.pkl"

            joblib.dump(scaler_input, input_scaler_file)
            joblib.dump(scaler_output, output_scaler_file)

            # zip ‡∏ó‡∏±‡πâ‡∏á folder ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
            shutil.make_archive(str(zip_filename.with_suffix('')), 'zip', tmpdir)

        print(f"‚úÖ Fine-tuned scalers saved to: {zip_filename}")

    except FileNotFoundError:
        print(f"‚ùå File  config scaler not found Error")
        return
    

def fine_tune_model():
    try:
        # üü¢ ‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å zip
        zip_files = list(Path(SCALER_FINE_TUNE_FOLDER).glob("*.zip"))
        if not zip_files:
            print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå zip ‡∏Ç‡∏≠‡∏á scaler")
            return

        latest_zip = max(zip_files, key=lambda p: p.stat().st_mtime)

        with tempfile.TemporaryDirectory() as tmpdir:
            shutil.unpack_archive(latest_zip, tmpdir, 'zip')
            tmpdir_path = Path(tmpdir)
            scaler_input = joblib.load(sorted(tmpdir_path.glob("scaler_input_*.pkl"))[0])
            scaler_output = joblib.load(sorted(tmpdir_path.glob("scaler_output_*.pkl"))[0])

        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print("‚úÖ Loaded scalers from zip and ready for fine-tuning")
        model = LSTMModel(input_dim=2,
                      hidden_dim=HIDDEN_SIZE,
                      layer_dim=NUM_LAYERS,
                      output_dim=1).to(device)

        criterion = nn.MSELoss()
        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
        start_time = time.time()
        model.train()
        csv_files = list(Path(NEW_DATA_FOLDER).glob("*.csv"))
        start_time = time.time()

        for epoch in range(NUM_EPOCHS):
            model.train()
            epoch_loss = 0.0
            batch_count = 0

            train_gen = data_generator(csv_files, scaler_input, scaler_output,
                                    WINDOW_SIZE, BATCH_SIZE)

            for X_batch, y_batch in train_gen:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)

                outputs = model(X_batch)
                loss = criterion(outputs, y_batch)

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                epoch_loss += loss.item()
                batch_count += 1

            avg_loss = epoch_loss / max(1, batch_count)
            print(f"Epoch [{epoch+1}/{NUM_EPOCHS}] - Loss: {avg_loss:.6f}")

        total_time = time.time() - start_time
        formatted_time = format_duration(total_time)
        print(f"‚úÖ Training finished. Total time: {formatted_time}")


        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
        timestamp = time.strftime("%d_%m_%Y")
        model_filename = Path(MODEL_FINE_TUNE_FOLDER) / f"lstm_model_{timestamp}.pth"
        torch.save(model.state_dict(), model_filename)
        print(f"üìÇ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ó‡∏µ‡πà {model_filename}")


    except FileNotFoundError:
        print(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏î‡∏¥‡∏°! ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏±‡∏ô‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå 02_train_model.py ‡∏Å‡πà‡∏≠‡∏ô")
        return


if __name__ == "__main__":
    fine_tune_scale()
    fine_tune_model()
