import torch
import torch.nn as nn

class LSTMForecaster(nn.Module):
    """
    LSTM ‡∏ó‡∏µ‡πà‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡πâ‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏á‡∏≤‡∏ô‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Time Series)
    ‡πÅ‡∏•‡∏∞‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡πà‡∏≠‡∏¢‡∏≠‡∏î‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡πÅ‡∏ö‡∏ö stateful ‡∏´‡∏£‡∏∑‡∏≠ RL-based Control ‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï
    """

    def __init__(self, input_dim, hidden_dim=64, num_layers=2, output_dim=1, dropout=0.1, stateful=False):
        super(LSTMForecaster, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.stateful = stateful

        self.lstm = nn.LSTM(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout
        )
        self.fc = nn.Linear(hidden_dim, output_dim)

        # ‡πÄ‡∏Å‡πá‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏ñ‡πâ‡∏≤ stateful=True
        self.hidden_state = None

    def reset_state(self, batch_size=1, device=None):
        """‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï hidden/cell state (‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏ï‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏° simulation ‡∏´‡∏£‡∏∑‡∏≠ episode ‡πÉ‡∏´‡∏°‡πà)"""
        if device is None:
            device = next(self.parameters()).device
        self.hidden_state = (
            torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=device),
            torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=device)
        )

    def forward(self, x):
        """
        x: (batch_size, seq_len, input_dim)
        output: (batch_size, output_dim)
        """
        if self.stateful and self.hidden_state is not None:
            out, self.hidden_state = self.lstm(x, self.hidden_state)
        else:
            out, _ = self.lstm(x)

        # ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤ timestep ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
        out = self.fc(out[:, -1, :])
        return out

    def detach_state(self):
        """‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏ï‡∏≠‡∏ô train ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏î gradient ‡∏à‡∏≤‡∏Å state ‡πÄ‡∏î‡∏¥‡∏° (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô gradient overflow)"""
        if self.hidden_state is not None:
            self.hidden_state = (
                self.hidden_state[0].detach(),
                self.hidden_state[1].detach()
            )


if __name__ == "__main__":
    # üß™ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á LSTMForecaster
    batch_size, seq_len, input_dim = 4, 10, 2
    model = LSTMForecaster(input_dim=input_dim, hidden_dim=32, output_dim=1, stateful=True)

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á input ‡∏à‡∏≥‡∏•‡∏≠‡∏á
    x = torch.randn(batch_size, seq_len, input_dim)

    # reset state ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°
    model.reset_state(batch_size=batch_size)

    # forward pass
    y = model(x)
    print("‚úÖ Output shape:", y.shape)
    print("Sample output:", y.detach().cpu().numpy().flatten()[:5])
