import torch
import torch.nn as nn
import pandas as pd
import numpy as np
from pathlib import Path
import joblib
import time
import os

# --- ‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏à‡∏≤‡∏Å‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡πÄ‡∏î‡∏¥‡∏° ---
# (‡πÉ‡∏ô‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå‡∏à‡∏£‡∏¥‡∏á ‡∏Ñ‡∏ß‡∏£‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏•‡∏≤‡∏™‡πÅ‡∏•‡∏∞‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå utils.py)
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
    def forward(self, x):
        out, _ = self.lstm(x)
        out = self.fc(out[:, -1, :])
        return out

def create_sequences(data, window_size):
    X, y = [], []
    for i in range(len(data) - window_size):
        X.append(data[i:(i + window_size)])
        y.append(data[i + window_size, 1])
    return np.array(X), np.array(y)

def data_generator(file_list, scaler_in, scaler_out, window_size, batch_size):
    for file_path in file_list:
        df = pd.read_csv(file_path)
        data = df[['DATA_INPUT', 'DATA_OUTPUT']].values.astype(np.float32)
        scaled_input = scaler_in.transform(data[:, 0].reshape(-1, 1))
        scaled_output = scaler_out.transform(data[:, 1].reshape(-1, 1))
        scaled_data = np.hstack([scaled_input, scaled_output])
        X, y = create_sequences(scaled_data, window_size)
        for i in range(0, len(X), batch_size):
            yield (torch.from_numpy(X[i:i+batch_size]).float(),
                   torch.from_numpy(y[i:i+batch_size]).float().view(-1, 1))

# --- ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fine-Tuning ---
SCALER_FOLDER = "scalers"
MODEL_FOLDER = "models"
NEW_DATA_FOLDER = "new_week_data" # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà

# Hyperparameters ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fine-Tuning
WINDOW_SIZE = 30
BATCH_SIZE = 64
FINE_TUNE_EPOCHS = 5 # Fine-tune ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ Epoch ‡πÄ‡∏¢‡∏≠‡∏∞
FINE_TUNE_LR = 0.0001 # *** ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å: ‡πÉ‡∏ä‡πâ Learning Rate ‡∏ó‡∏µ‡πà‡∏ï‡πà‡∏≥‡∏•‡∏á ***

# --- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Fine-Tune ---
def fine_tune_weekly_model():
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ Fine-Tuning ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    """
    print("--- üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ Fine-Tuning ‡∏£‡∏≤‡∏¢‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå ---")
    
    # --- ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Scaler ‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà ---
    print("\n--- [‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1/2] ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Scaler... ---")
    try:
        # 1.1 ‡πÇ‡∏´‡∏•‡∏î Scaler ‡πÄ‡∏Å‡πà‡∏≤
        input_scaler_path = Path(SCALER_FOLDER) / "global_scaler_input.pkl"
        output_scaler_path = Path(SCALER_FOLDER) / "global_scaler_output.pkl"
        scaler_input = joblib.load(input_scaler_path)
        scaler_output = joblib.load(output_scaler_path)
        print("  -> ‡πÇ‡∏´‡∏•‡∏î Scaler ‡πÄ‡∏î‡∏¥‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")

        # 1.2 ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà
        new_files = list(Path(NEW_DATA_FOLDER).glob("*.csv"))
        if not new_files:
            print(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå '{NEW_DATA_FOLDER}', ‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô")
            return
        print(f"  -> ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà {len(new_files)} ‡πÑ‡∏ü‡∏•‡πå")

        # 1.3 ‡∏ó‡∏≥ partial_fit ‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà
        for file_path in new_files:
            with pd.read_csv(file_path, chunksize=10000) as reader:
                for chunk in reader:
                    scaler_input.partial_fit(chunk[['DATA_INPUT']].values)
                    scaler_output.partial_fit(chunk[['DATA_OUTPUT']].values)
        print("  -> ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Scaler ‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")

        # 1.4 ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Scaler ‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏¥‡∏°
        joblib.dump(scaler_input, input_scaler_path)
        joblib.dump(scaler_output, output_scaler_path)
        print("  -> ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Scaler ‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏±‡∏ö‡∏Ç‡∏≠‡∏á‡πÄ‡∏î‡∏¥‡∏°")

    except FileNotFoundError:
        print(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå Scaler ‡πÄ‡∏î‡∏¥‡∏°! ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏±‡∏ô‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå 01_prepare_scaler.py ‡∏Å‡πà‡∏≠‡∏ô")
        return

    # --- ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2: ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏Å‡πà‡∏≤‡πÅ‡∏•‡∏∞‡∏ù‡∏∂‡∏Å‡∏ï‡πà‡∏≠‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà ---
    print("\n--- [‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2/2] ‡∏Å‡∏≥‡∏•‡∏±‡∏á Fine-Tune ‡πÇ‡∏°‡πÄ‡∏î‡∏•... ---")
    try:
        # 2.1 ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞‡πÇ‡∏´‡∏•‡∏î state ‡∏à‡∏≤‡∏Å‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå‡∏ó‡∏µ‡πà‡πÅ‡∏•‡πâ‡∏ß
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model_path = Path(MODEL_FOLDER) / "lstm_model.pth"
        
        model = LSTMModel(input_size=2, hidden_size=50, num_layers=1, output_size=1).to(device)
        model.load_state_dict(torch.load(model_path, map_location=device))
        print(f"  -> ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏≤‡∏Å '{model_path}' ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")

        # 2.2 ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Optimizer ‡∏î‡πâ‡∏ß‡∏¢ Learning Rate ‡∏ó‡∏µ‡πà‡∏ï‡πà‡∏≥‡∏•‡∏á
        optimizer = torch.optim.Adam(model.parameters(), lr=FINE_TUNE_LR)
        criterion = nn.MSELoss()
        
        # 2.3 ‡πÄ‡∏£‡∏¥‡πà‡∏° Fine-Tuning Loop (‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà)
        print(f"  -> ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£ Fine-Tune ‡∏î‡πâ‡∏ß‡∏¢ Learning Rate = {FINE_TUNE_LR}")
        start_time = time.time()
        model.train()
        
        for epoch in range(FINE_TUNE_EPOCHS):
            total_loss = 0
            num_batches = 0
            
            # Generator ‡∏à‡∏∞‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà
            fine_tune_gen = data_generator(new_files, scaler_input, scaler_output, WINDOW_SIZE, BATCH_SIZE)

            for X_batch, y_batch in fine_tune_gen:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                
                outputs = model(X_batch)
                loss = criterion(outputs, y_batch)
                
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
                num_batches += 1
            
            avg_loss = total_loss / num_batches if num_batches > 0 else 0
            print(f'    Epoch [{epoch+1:02d}/{FINE_TUNE_EPOCHS}], Loss: {avg_loss:.6f}')
        
        # 2.4 ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏¥‡∏°
        torch.save(model.state_dict(), model_path)
        total_time = time.time() - start_time
        print(f"  -> ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏±‡∏ö‡∏Ç‡∏≠‡∏á‡πÄ‡∏î‡∏¥‡∏°‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")
        print(f"\n‚úÖ Fine-Tuning ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤: {total_time:.2f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ")

    except FileNotFoundError:
        print(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏î‡∏¥‡∏°! ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏±‡∏ô‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå 02_train_model.py ‡∏Å‡πà‡∏≠‡∏ô")
        return

# --- ‡∏™‡∏±‡πà‡∏á‡πÉ‡∏´‡πâ‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏ó‡∏≥‡∏á‡∏≤‡∏ô ---
if __name__ == "__main__":
    # --- ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå ---
    # 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà (‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ)
    Path("new_week_data").mkdir(exist_ok=True)
    
    # 2. ‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà"
    #    ‡πÉ‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏à‡∏£‡∏¥‡∏á ‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
    #    (‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå raw)
    try:
        from shutil import copy
        if not os.path.exists("new_week_data/new_data_18_09_2568.csv"):
             copy("raw/data_log_simulation_17_09_2568.csv", "new_week_data/new_data_18_09_2568.csv")
    except FileNotFoundError:
        print("‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå 'raw/data_log_simulation_17_09_2568.csv' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà")
    
    # 3. ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ Fine-Tune
    fine_tune_weekly_model()