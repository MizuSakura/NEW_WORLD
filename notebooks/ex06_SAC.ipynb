{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33b254dd",
   "metadata": {},
   "source": [
    "Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b8d1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Logger:\n",
    "    \"\"\"\n",
    "    Simple logger for collecting training metrics during SAC training.\n",
    "    Stores each metric as a list so they can be plotted later.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.data = {}\n",
    "\n",
    "    def log(self, key, value):\n",
    "        \"\"\"Append a value to a specific metric key.\"\"\"\n",
    "        if key not in self.data:\n",
    "            self.data[key] = []\n",
    "        self.data[key].append(value)\n",
    "\n",
    "    def get(self, key):\n",
    "        \"\"\"Return all logged values of a metric.\"\"\"\n",
    "        return self.data.get(key, [])\n",
    "\n",
    "    def keys(self):\n",
    "        \"\"\"Return all metric names.\"\"\"\n",
    "        return list(self.data.keys())\n",
    "\n",
    "    def summary(self, last_n=10):\n",
    "        \"\"\"Print summary of mean of the last N entries.\"\"\"\n",
    "        print(\"=== Logger Summary ===\")\n",
    "        for k, v in self.data.items():\n",
    "            if len(v) > 0:\n",
    "                print(f\"{k}: mean(last {last_n}) = {np.mean(v[-last_n:]):.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298b173c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger()\n",
    "\n",
    "# Example logging\n",
    "for i in range(5):\n",
    "    logger.log(\"loss_actor\", np.random.random())\n",
    "    logger.log(\"entropy\", np.random.random() * 0.5)\n",
    "\n",
    "logger.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36644f73",
   "metadata": {},
   "source": [
    "Actor Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a2ca1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"\n",
    "    Stochastic Actor Network for SAC (Gaussian Policy with Tanh Squash)\n",
    "    Supports per-dimension action ranges, e.g.\n",
    "    min_action = [-1, -2, -0.5]\n",
    "    max_action = [ 1,  2,  1.0]\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, min_action, max_action):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # Ensure min/max are tensors with correct shape\n",
    "        min_action = torch.tensor(min_action, dtype=torch.float32)\n",
    "        max_action = torch.tensor(max_action, dtype=torch.float32)\n",
    "\n",
    "        assert min_action.shape == (action_dim,)\n",
    "        assert max_action.shape == (action_dim,)\n",
    "\n",
    "        # Register as buffers (moved automatically with model.to(device))\n",
    "        self.register_buffer(\"min_action\", min_action)\n",
    "        self.register_buffer(\"max_action\", max_action)\n",
    "\n",
    "        # Actor network\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 256), nn.ReLU(),\n",
    "            nn.Linear(256, action_dim),\n",
    "        )\n",
    "\n",
    "        # Learnable log standard deviation\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "\n",
    "        # Limit std range for stability\n",
    "        self.LOG_STD_MIN = -20\n",
    "        self.LOG_STD_MAX = 2\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Returns mean and std of the Gaussian policy BEFORE tanh squash.\n",
    "        \"\"\"\n",
    "        mean = self.net(state)\n",
    "\n",
    "        # Clamp log_std for stability\n",
    "        log_std = torch.clamp(self.log_std, self.LOG_STD_MIN, self.LOG_STD_MAX)\n",
    "        std = torch.exp(log_std)\n",
    "\n",
    "        return mean, std\n",
    "\n",
    "    def sample(self, state):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        - scaled action (after tanh + min/max scaling)\n",
    "        - log_prob of the action (required for SAC)\n",
    "        \"\"\"\n",
    "        mean, std = self.forward(state)\n",
    "        dist = Normal(mean, std)\n",
    "\n",
    "        # Reparameterization trick\n",
    "        x_t = dist.rsample()\n",
    "        y_t = torch.tanh(x_t)\n",
    "\n",
    "        # Scale [-1,1] → [min,max]\n",
    "        action = self.min_action + (y_t + 1) * 0.5 * (self.max_action - self.min_action)\n",
    "\n",
    "        # Log probability with tanh correction\n",
    "        log_prob = dist.log_prob(x_t)\n",
    "        log_prob -= torch.log(1 - y_t.pow(2) + 1e-6)\n",
    "        log_prob = torch.sum(log_prob, dim=1, keepdim=True)\n",
    "\n",
    "        return action, log_prob\n",
    "\n",
    "    def sample_deterministic(self, state):\n",
    "        \"\"\"\n",
    "        Returns deterministic action (mean → tanh → scaled)\n",
    "        \"\"\"\n",
    "        mean, _ = self.forward(state)\n",
    "        y_t = torch.tanh(mean)\n",
    "\n",
    "        action = self.min_action + (y_t + 1) * 0.5 * (self.max_action - self.min_action)\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf61026",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = 5\n",
    "action_dim = 3\n",
    "min_action = [0, 0, 0.5]\n",
    "max_action = [1, 2, 1]\n",
    "\n",
    "# สร้าง Actor\n",
    "actor = Actor(state_dim, action_dim, min_action, max_action)\n",
    "\n",
    "# ส่งไป GPU ถ้ามี\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "actor.to(device)\n",
    "\n",
    "# สร้าง dummy state เพื่อทดสอบ\n",
    "state = torch.randn(1, state_dim).to(device)  # batch size 1\n",
    "action, log_prob = actor.sample(state)\n",
    "\n",
    "print(\"Sampled action:\\n\", action)\n",
    "print(\"Log probability:\\n\", log_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3439ac77",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4541ec7",
   "metadata": {},
   "source": [
    "Critic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8631bd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"\n",
    "    Twin Q Network for SAC (Q1 and Q2)\n",
    "    Inputs:\n",
    "        state:  [batch, state_dim]\n",
    "        action: [batch, action_dim]\n",
    "    Output:\n",
    "        q1, q2: [batch, 1]\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # Q1 network\n",
    "        self.q1_net = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "        # Q2 network\n",
    "        self.q2_net = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "        # Apply weight initialization\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        \"\"\"\n",
    "        Custom weight initialization for better stability.\n",
    "        \"\"\"\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"\n",
    "        Forward both Q networks.\n",
    "        \"\"\"\n",
    "        # Ensure concatenation on correct dimension\n",
    "        sa = torch.cat([state, action], dim=1)\n",
    "\n",
    "        q1 = self.q1_net(sa)\n",
    "        q2 = self.q2_net(sa)\n",
    "        return q1, q2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343dc910",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = 5\n",
    "action_dim = 3\n",
    "\n",
    "critic = Critic(state_dim, action_dim)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "critic.to(device)\n",
    "\n",
    "# Dummy state & action\n",
    "state = torch.randn(1, state_dim).to(device)  # batch size 1\n",
    "action = torch.randn(1, action_dim).to(device)\n",
    "\n",
    "q1, q2 = critic(state, action)\n",
    "print(\"Q1:\\n\", q1)\n",
    "print(\"Q2:\\n\", q2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d847b62",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a351e32",
   "metadata": {},
   "source": [
    "Replaybuffer memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cc086e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class Vanilla_ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Standard Replay Buffer for off-policy RL (SAC, TD3, DDPG)\n",
    "    Features:\n",
    "    - Fast NumPy storage\n",
    "    - Tensor conversion only during sampling\n",
    "    - Supports large batches efficiently\n",
    "    - Works with GPU via device argument\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, capacity=100000, device='cpu'):\n",
    "        self.capacity = capacity\n",
    "        self.device = device\n",
    "\n",
    "        self.ptr = 0      # write pointer\n",
    "        self.size = 0     # current size\n",
    "\n",
    "        # Pre-allocate memory (fast!)\n",
    "        self.state = np.zeros((capacity, state_dim), dtype=np.float32)\n",
    "        self.action = np.zeros((capacity, action_dim), dtype=np.float32)\n",
    "        self.reward = np.zeros((capacity, 1), dtype=np.float32)\n",
    "        self.next_state = np.zeros((capacity, state_dim), dtype=np.float32)\n",
    "        self.done = np.zeros((capacity, 1), dtype=np.float32)\n",
    "\n",
    "    def push(self, s, a, r, s2, d):\n",
    "        \"\"\"\n",
    "        Store one transition (s, a, r, s2, done)\n",
    "        \"\"\"\n",
    "        i = self.ptr\n",
    "\n",
    "        self.state[i] = s\n",
    "        self.action[i] = a\n",
    "        self.reward[i] = r\n",
    "        self.next_state[i] = s2\n",
    "        self.done[i] = d\n",
    "\n",
    "        # Move pointer\n",
    "        self.ptr = (self.ptr + 1) % self.capacity\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Random sample a mini-batch.\n",
    "        Returns tensors on the selected device.\n",
    "        \"\"\"\n",
    "        idx = np.random.randint(0, self.size, size=batch_size)\n",
    "\n",
    "        state      = torch.tensor(self.state[idx], dtype=torch.float32, device=self.device)\n",
    "        action     = torch.tensor(self.action[idx], dtype=torch.float32, device=self.device)\n",
    "        reward     = torch.tensor(self.reward[idx], dtype=torch.float32, device=self.device)\n",
    "        next_state = torch.tensor(self.next_state[idx], dtype=torch.float32, device=self.device)\n",
    "        done       = torch.tensor(self.done[idx], dtype=torch.float32, device=self.device)\n",
    "\n",
    "        return state, action, reward, next_state, done\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d50d88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = 4\n",
    "action_dim = 2\n",
    "buffer = Vanilla_ReplayBuffer(state_dim, action_dim, capacity=10, device='cpu')\n",
    "\n",
    "# สร้าง dummy data\n",
    "for i in range(12):  # push เกิน capacity เพื่อทดสอบ circular buffer\n",
    "    s = np.random.randn(state_dim)\n",
    "    a = np.random.randn(action_dim)\n",
    "    r = np.random.randn(1)\n",
    "    s2 = np.random.randn(state_dim)\n",
    "    d = np.random.randint(0,2)\n",
    "    buffer.push(s, a, r, s2, d)\n",
    "    print(f\"Pushed transition {i+1}, buffer size: {len(buffer)}\")\n",
    "\n",
    "# sample batch\n",
    "batch_size = 5\n",
    "s_batch, a_batch, r_batch, s2_batch, d_batch = buffer.sample(batch_size)\n",
    "\n",
    "print(\"\\nSampled states:\\n\", s_batch)\n",
    "print(\"Sampled actions:\\n\", a_batch)\n",
    "print(\"Sampled rewards:\\n\", r_batch)\n",
    "print(\"Sampled next_states:\\n\", s2_batch)\n",
    "print(\"Sampled dones:\\n\", d_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567ee19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = 4\n",
    "action_dim = 2\n",
    "buffer = Vanilla_ReplayBuffer(state_dim, action_dim, capacity=5)\n",
    "\n",
    "# Push 3 transitions\n",
    "for i in range(3):\n",
    "    buffer.push(np.random.randn(state_dim), np.random.randn(action_dim), np.random.randn(1),\n",
    "                np.random.randn(state_dim), 0)\n",
    "    print(f\"After push {i+1}:\")\n",
    "    print(\"len(buffer):\", len(buffer))\n",
    "    print(\"buffer.__len__():\", buffer.__len__())\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d2d62a",
   "metadata": {},
   "source": [
    "SAC Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78daa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "\n",
    "class SACAgent:\n",
    "    \"\"\"\n",
    "    Soft Actor-Critic (SAC) Agent\n",
    "    ------------------------------------------------------------\n",
    "    A clean and minimal SAC implementation suitable for research,\n",
    "    simulation studies, and real-world control tasks.\n",
    "\n",
    "    Key Features\n",
    "    ------------\n",
    "    - Stochastic Gaussian Actor (Tanh-squashed)\n",
    "    - Twin Q-Critic network (Q1, Q2) to avoid overestimation\n",
    "    - Target Critic for stable Bellman backups\n",
    "    - Fixed entropy coefficient α (no automatic entropy tuning)\n",
    "    - Works with a standard Replay Buffer\n",
    "    - Optional logging for training curves\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state_dim : int\n",
    "        Dimensionality of the state vector.\n",
    "    action_dim : int\n",
    "        Dimensionality of the action vector.\n",
    "    min_action, max_action : float or array-like\n",
    "        Action bounds after Tanh scaling.\n",
    "    lr : float\n",
    "        Learning rate for both actor and critic optimizers.\n",
    "    gamma : float\n",
    "        Discount factor used in the target Q-value computation.\n",
    "    tau : float\n",
    "        Soft update coefficient for the target critic.\n",
    "    alpha : float\n",
    "        Entropy regularization coefficient (higher = more exploration).\n",
    "    replay_capacity : int\n",
    "        Maximum size of the replay buffer.\n",
    "    device : str\n",
    "        'cpu' or 'cuda'.\n",
    "    logger_status : bool\n",
    "        Whether to enable metric logging during training.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> agent = SACAgent(state_dim=3, action_dim=1,\n",
    "    ...                  min_action=-1, max_action=1,\n",
    "    ...                  logger_status=True)\n",
    "    >>> state = env.reset()\n",
    "    >>> action = agent.select_action(state)\n",
    "    >>> agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "    >>> agent.update(batch_size=64)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, min_action, max_action, \n",
    "                 lr=3e-4, gamma=0.99, tau=0.005, alpha=0.2,\n",
    "                 replay_capacity=100000,\n",
    "                 device='cuda',\n",
    "                 logger_status=False):\n",
    "        \n",
    "        # Device selection with CUDA fallback\n",
    "        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Initialize Actor, Critic, and Target Critic Networks\n",
    "        # Target critic starts as a direct copy of the critic.\n",
    "        # ------------------------------------------------------------\n",
    "        self.actor = Actor(state_dim, action_dim, min_action, max_action).to(self.device)\n",
    "        self.critic = Critic(state_dim, action_dim).to(self.device)\n",
    "        self.target_critic = Critic(state_dim, action_dim).to(self.device)\n",
    "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Optimizers for Actor and Critic\n",
    "        # ------------------------------------------------------------\n",
    "        self.actor_opt = torch.optim.Adam(self.actor.parameters(), lr=lr)\n",
    "        self.critic_opt = torch.optim.Adam(self.critic.parameters(), lr=lr)\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Replay Buffer\n",
    "        # ------------------------------------------------------------\n",
    "        self.replay_buffer = Vanilla_ReplayBuffer(\n",
    "            state_dim, action_dim,\n",
    "            capacity=replay_capacity,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Hyperparameters\n",
    "        # ------------------------------------------------------------\n",
    "        self.gamma = gamma      # discount factor\n",
    "        self.tau = tau          # target network soft update rate\n",
    "        self.alpha = alpha      # entropy weight (encourages exploration)\n",
    "\n",
    "        # Logger for training curves\n",
    "        self.logger = Logger()\n",
    "        self.logger_status = logger_status\n",
    "\n",
    "    # ======================================================================\n",
    "    # Action Selection\n",
    "    # ======================================================================\n",
    "    def select_action(self, state, deterministic=False):\n",
    "        \"\"\"\n",
    "        Select an action given the environment state.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        deterministic : bool\n",
    "            - True:  use the mean action (good for evaluation)\n",
    "            - False: sample stochastically (recommended for training)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        action : np.ndarray\n",
    "            Scaled action in the range [min_action, max_action].\n",
    "        \"\"\"\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "\n",
    "        if deterministic:\n",
    "            # Use the actor mean → tanh → scale back to action bounds.\n",
    "            mean, _ = self.actor.forward(state)\n",
    "            y_t = torch.tanh(mean)\n",
    "            action = self.actor.min_action + (y_t + 1) * 0.5 * (self.actor.max_action - self.actor.min_action)\n",
    "        else:\n",
    "            # Sample using reparameterization trick.\n",
    "            # Produces differentiable stochastic actions.\n",
    "            action, _ = self.actor.sample(state)\n",
    "\n",
    "        return action.cpu().detach().numpy()[0]\n",
    "\n",
    "    # ======================================================================\n",
    "    # Training Step\n",
    "    # ======================================================================\n",
    "    def update(self, batch_size=64):\n",
    "        \"\"\"\n",
    "        Performs one SAC update step:\n",
    "        - Sample batch from replay buffer\n",
    "        - Update critic using Bellman backup\n",
    "        - Update actor by maximizing expected Q minus entropy\n",
    "        - Soft update target critic\n",
    "\n",
    "        This function is called repeatedly during training.\n",
    "        \"\"\"\n",
    "        if len(self.replay_buffer) < batch_size:\n",
    "            return  # Not enough samples yet\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Sample a random mini-batch from the Replay Buffer\n",
    "        # ------------------------------------------------------------\n",
    "        state, action, reward, next_state, done = self.replay_buffer.sample(batch_size)\n",
    "        state, action, reward, next_state, done = [\n",
    "            x.to(self.device) for x in (state, action, reward, next_state, done)\n",
    "        ]\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Compute the Target Q-value (Bootstrapped)\n",
    "        # ------------------------------------------------------------\n",
    "        with torch.no_grad():\n",
    "            next_action, next_log_prob = self.actor.sample(next_state)\n",
    "            q1_t, q2_t = self.target_critic(next_state, next_action)\n",
    "\n",
    "            # Twin-critic trick: use min(Q1, Q2) to reduce positive bias\n",
    "            min_q_t = torch.min(q1_t, q2_t)\n",
    "\n",
    "            # SAC target: r + γ * (minQ - α logπ)\n",
    "            target_q = reward + (1 - done) * self.gamma * (min_q_t - self.alpha * next_log_prob)\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Critic Update (Minimize MSE between Q and target)\n",
    "        # ------------------------------------------------------------\n",
    "        q1, q2 = self.critic(state, action)\n",
    "\n",
    "        critic_loss = F.mse_loss(q1, target_q) + F.mse_loss(q2, target_q)\n",
    "\n",
    "        self.critic_opt.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_opt.step()\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Actor Update (Maximize expected Q - α * entropy)\n",
    "        # ------------------------------------------------------------\n",
    "        a_pi, log_pi = self.actor.sample(state)\n",
    "        q1_pi, q2_pi = self.critic(state, a_pi)\n",
    "        q_pi = torch.min(q1_pi, q2_pi)\n",
    "\n",
    "        # Actor minimizes (α logπ - Q) which equals maximizing Q - α logπ\n",
    "        actor_loss = (self.alpha * log_pi - q_pi).mean()\n",
    "\n",
    "        self.actor_opt.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_opt.step()\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Soft Update for Target Critic\n",
    "        # θ_target ← τ θ + (1 - τ) θ_target\n",
    "        # This stabilizes learning by slowly tracking the critic.\n",
    "        # ------------------------------------------------------------\n",
    "        for p, tp in zip(self.critic.parameters(), self.target_critic.parameters()):\n",
    "            tp.data.copy_(self.tau * p.data + (1 - self.tau) * tp.data)\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Logging (losses, entropy, Q values, etc.)\n",
    "        # ------------------------------------------------------------\n",
    "        if self.logger_status:\n",
    "            self.logger.log(\"loss_actor\", actor_loss.item())\n",
    "            self.logger.log(\"loss_critic\", critic_loss.item())\n",
    "            self.logger.log(\"q1_mean\", q1.mean().item())\n",
    "            self.logger.log(\"q2_mean\", q2.mean().item())\n",
    "            self.logger.log(\"entropy\", -log_pi.mean().item())\n",
    "            self.logger.log(\"alpha\", self.alpha)\n",
    "            self.logger.log(\"tau\", self.tau)\n",
    "    \n",
    "    # ======================================================================\n",
    "    # Save / Load Model (Drop-Version Safe, pathlib + auto .pt)\n",
    "    # ======================================================================\n",
    "    def save_model(self, path=\"sac_model.pt\"):\n",
    "        \"\"\"\n",
    "        Save Actor, Critic, Target Critic networks and hyperparameters\n",
    "        into a single file using state_dict.\n",
    "        Drop-version safe (compatible across PyTorch versions).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path : str or Path\n",
    "            File path for saving the model.\n",
    "            Example: \"checkpoints/sac_model\" or \"checkpoints/sac_model.pt\"\n",
    "        \"\"\"\n",
    "        path = Path(path)\n",
    "        if path.suffix != \".pt\":\n",
    "            path = path.with_suffix(\".pt\")  # Add .pt if missing\n",
    "\n",
    "        # Ensure parent directories exist\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        data = {\n",
    "            \"actor\": self.actor.state_dict(),\n",
    "            \"critic\": self.critic.state_dict(),\n",
    "            \"target_critic\": self.target_critic.state_dict(),\n",
    "            \"hyperparams\": {\n",
    "                \"gamma\": self.gamma,\n",
    "                \"tau\": self.tau,\n",
    "                \"alpha\": self.alpha,\n",
    "                \"min_action\": self.actor.min_action.cpu().tolist(),\n",
    "                \"max_action\": self.actor.max_action.cpu().tolist(),\n",
    "                \"state_dim\": self.actor.state_dim,\n",
    "                \"action_dim\": self.actor.action_dim\n",
    "            }\n",
    "        }\n",
    "        torch.save(data, path)\n",
    "        print(f\"[SACAgent] Model saved to '{path}'\")\n",
    "\n",
    "    def load_model(self, path=\"sac_model.pt\"):\n",
    "        \"\"\"\n",
    "        Load Actor, Critic, Target Critic networks and hyperparameters\n",
    "        from a single drop-version-safe file.\n",
    "        \"\"\"\n",
    "        path = Path(path)\n",
    "        if path.suffix != \".pt\":\n",
    "            path = path.with_suffix(\".pt\")\n",
    "\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f\"[SACAgent] File not found: '{path}'\")\n",
    "\n",
    "        # Safe loader for drop-version compatibility\n",
    "        data = torch.load(path, map_location=self.device)\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # Helper: safely convert anything → Tensor without PyTorch warning\n",
    "        # ------------------------------------------------------------------\n",
    "        def to_tensor_safe(x):\n",
    "            if isinstance(x, torch.Tensor):\n",
    "                return x.detach().clone().float()\n",
    "            return torch.as_tensor(x, dtype=torch.float32)\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # 1) Load hyperparameters\n",
    "        # ------------------------------------------------------------------\n",
    "        hyper = data[\"hyperparams\"]\n",
    "        self.gamma = hyper[\"gamma\"]\n",
    "        self.tau = hyper[\"tau\"]\n",
    "        self.alpha = hyper[\"alpha\"]\n",
    "\n",
    "        min_action = to_tensor_safe(hyper[\"min_action\"])\n",
    "        max_action = to_tensor_safe(hyper[\"max_action\"])\n",
    "\n",
    "        state_dim  = hyper[\"state_dim\"]\n",
    "        action_dim = hyper[\"action_dim\"]\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # 2) Re-create Actor (must match original architecture)\n",
    "        # ------------------------------------------------------------------\n",
    "        self.actor = Actor(\n",
    "            state_dim=state_dim,\n",
    "            action_dim=action_dim,\n",
    "            min_action=min_action,\n",
    "            max_action=max_action\n",
    "        ).to(self.device)\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # 3) Load network weights (drop-version safe)\n",
    "        # ------------------------------------------------------------------\n",
    "        self.actor.load_state_dict(data[\"actor\"], strict=False)\n",
    "        self.critic.load_state_dict(data[\"critic\"])\n",
    "        self.target_critic.load_state_dict(data[\"target_critic\"])\n",
    "\n",
    "        print(f\"[SACAgent] Model loaded from '{path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8972ea30",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791246a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# สร้าง environment\n",
    "env = gym.make(\"Pendulum-v1\", render_mode=\"human\")\n",
    "\n",
    "state, _ = env.reset()\n",
    "state_dim = state.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "min_action = env.action_space.low\n",
    "max_action = env.action_space.high\n",
    "\n",
    "print(\"State dim:\", state_dim)\n",
    "print(\"Action dim:\", action_dim)\n",
    "print(\"Action range:\", min_action, max_action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a33d13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# สร้าง agent\n",
    "agent = SACAgent(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    min_action=min_action,\n",
    "    max_action=max_action,\n",
    "    lr=3e-4,\n",
    "    gamma=0.99,\n",
    "    tau=0.005,\n",
    "    alpha=0.4,\n",
    "    logger_status= True\n",
    ")\n",
    "\n",
    "# ===== Training Parameters =====\n",
    "episodes = 500\n",
    "max_steps = 200\n",
    "batch_size = 512\n",
    "\n",
    "rewards_history = []\n",
    "\n",
    "# ===== Training Loop =====\n",
    "for ep in range(episodes):\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    episode_reward = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        # 1. เลือก action\n",
    "        action = agent.select_action(state)\n",
    "       \n",
    "\n",
    "        # 2. Step environment\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # 3. เก็บลง Replay Buffer\n",
    "        agent.replay_buffer.push(\n",
    "            state, action, reward, next_state, float(done)\n",
    "        )\n",
    "\n",
    "        # 4. อัปเดต network\n",
    "        agent.update(batch_size)\n",
    "        \n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    rewards_history.append(episode_reward)\n",
    "    print(f\"Episode {ep+1}/{episodes} Reward: {episode_reward:.2f}\")\n",
    "agent.save_model(path=rf\"D:\\Project_end\\New_world\\my_project\\notebooks\\Test_Pendulum-v1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a839c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Replay buffer size:\", len(agent.replay_buffer))\n",
    "print(\"q1|q2 size: \",len(agent.logger.get(\"q1_mean\")), len(agent.logger.get(\"q2_mean\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c785f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(rewards_history)\n",
    "plt.title(\"Episode Reward\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4230704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(agent.logger.get(\"loss_actor\"), label=\"Actor Loss\",alpha=0.5)\n",
    "plt.plot(agent.logger.get(\"loss_critic\"), label=\"Critic Loss\",alpha=0.5)\n",
    "plt.title(\"Loss Curve\")\n",
    "plt.xlabel(\"Training Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "# ------ Left : Actor Loss ------\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(agent.logger.get(\"loss_actor\"), label=\"Actor Loss\", alpha=0.7)\n",
    "plt.title(\"Actor Loss\")\n",
    "plt.xlabel(\"Training Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# ------ Right : Critic Loss ------\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(agent.logger.get(\"loss_critic\"), label=\"Critic Loss\", alpha=0.7, color=\"orange\")\n",
    "plt.title(\"Critic Loss\")\n",
    "plt.xlabel(\"Training Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ac86ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(agent.logger.get(\"q1_mean\"), label=\"Q1 Mean\",alpha=0.3)\n",
    "plt.plot(agent.logger.get(\"q2_mean\"), label=\"Q2 Mean\",alpha=0.3)\n",
    "plt.title(\"Q-value Estimate\")\n",
    "plt.xlabel(\"Training Step\")\n",
    "plt.ylabel(\"Q-value\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "# ------ Left : Actor Loss ------\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(agent.logger.get(\"q1_mean\"), label=\"Q1 Mean\",alpha=0.3)\n",
    "plt.title(\"Actor Loss\")\n",
    "plt.xlabel(\"Training Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# ------ Right : Critic Loss ------\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(agent.logger.get(\"q2_mean\"), label=\"Q2 Mean\",alpha=0.3)\n",
    "plt.title(\"Critic Loss\")\n",
    "plt.xlabel(\"Training Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7839a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(agent.logger.get(\"entropy\"), label=\"Entropy\")\n",
    "plt.title(\"Policy Entropy\")\n",
    "plt.xlabel(\"Training Step\")\n",
    "plt.ylabel(\"Entropy\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7f54c7",
   "metadata": {},
   "source": [
    "Test Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb3c60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(env, agent, episodes=5, max_steps=200):\n",
    "    returns = []\n",
    "    trajectories = []   # เก็บ state/action/reward เพื่อนำไป plot\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # ใช้ deterministic เพื่อดู performance จริง\n",
    "            action = agent.select_action(state, deterministic=True)\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # เก็บข้อมูลสำหรับ plot\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        returns.append(episode_reward)\n",
    "        trajectories.append({\n",
    "            \"states\": np.array(states),\n",
    "            \"actions\": np.array(actions),\n",
    "            \"rewards\": np.array(rewards),\n",
    "        })\n",
    "\n",
    "        print(f\"[TEST] Episode {ep+1}/{episodes} Reward = {episode_reward:.2f}\")\n",
    "\n",
    "    return returns, trajectories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f604dc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load_model(path=rf\"D:\\Project_end\\New_world\\my_project\\notebooks\\Test_Pendulum-v1.pt\")\n",
    "test_returns, test_traj = test_agent(env, agent, episodes=5, max_steps=200)\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(test_traj[2][\"rewards\"], label=\"Reward per step\")\n",
    "plt.title(\"Reward curve (Test Episode 1)\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c8755d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(test_traj[1][\"actions\"], label=\"Action\", alpha=0.7)\n",
    "plt.title(\"Action output (Test Episode 1)\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Action Value\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de009df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(test_traj[2][\"states\"])\n",
    "plt.title(\"State trajectory (Test Episode 1)\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"State Value\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e087a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(test_returns, marker=\"o\")\n",
    "plt.title(\"Total test return per episode\")\n",
    "plt.xlabel(\"Test Episode\")\n",
    "plt.ylabel(\"Total Return\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90186287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import SAC\n",
    "\n",
    "env = gym.make(\"MountainCarContinuous-v0\")\n",
    "\n",
    "model = SAC(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    learning_rate=3e-4,          # ไม่สูง ไม่ต่ำ (ตามงานวิจัย MC)\n",
    "    buffer_size=200000,          # สำคัญมาก ให้ agent explore นานๆ\n",
    "    learning_starts=5000,        # ต้องมีประสบการณ์พอ ก่อนเริ่ม learning\n",
    "\n",
    "    batch_size=64,               # เล็ก = stable กว่า\n",
    "    tau=0.02,                    # target update เร็วขึ้น เพิ่ม stability\n",
    "    gamma=0.98,                  # MountainCar horizon สั้น\n",
    "\n",
    "    train_freq=1,\n",
    "    gradient_steps=1,\n",
    "\n",
    "    # ปิด auto entropy → ต้องใช้ fixed entropy coefficient\n",
    "    ent_coef=0.1,                # สูตรสำเร็จ: 0.05–0.15\n",
    "                                # exploration พอดีสำหรับสร้าง momentum\n",
    "\n",
    "    # ลด network size → train เร็วกว่า หา optimal ได้ง่ายกว่า\n",
    "    policy_kwargs=dict(\n",
    "        net_arch=dict(\n",
    "            pi=[64, 64],         # actor network\n",
    "            qf=[64, 64]          # critic network\n",
    "        ),\n",
    "        log_std_init=-1.0,       # ทำให้ Gaussian กว้างพอในช่วงต้น\n",
    "    ),\n",
    "\n",
    "    target_update_interval=1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.learn(total_timesteps=300_000)\n",
    "model.save(\"sac_mountaincar_success\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d71d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"MountainCarContinuous-v0\", render_mode=\"human\")\n",
    "model = SAC.load(\"sac_mountaincar_success\")\n",
    "\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NEW_WORLD_SETUP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
