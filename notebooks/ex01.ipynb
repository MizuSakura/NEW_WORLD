{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08961232",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "âŒ à¹„à¸¡à¹ˆà¸žà¸šà¹„à¸Ÿà¸¥à¹Œ ZIP: D:\\Project_end\\New_world\\my_project\\config\\RC_Tank_Env_Training2_scalers.zip",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# ðŸ§ª à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¸à¸²à¸£à¹ƒà¸Šà¹‰à¸‡à¸²à¸™\u001b[39;00m\n\u001b[32m     40\u001b[39m     zip_file = \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mD:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mProject_end\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mNew_world\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mmy_project\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mRC_Tank_Env_Training2_scalers.zip\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     loader = \u001b[43mScalingZipLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzip_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m     scaler_in, scaler_out, metadata = loader.load()\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mðŸŽ¯ Metadata summary:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mScalingZipLoader.__init__\u001b[39m\u001b[34m(self, zip_path)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mself\u001b[39m.zip_path = Path(zip_path)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.zip_path.exists():\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâŒ à¹„à¸¡à¹ˆà¸žà¸šà¹„à¸Ÿà¸¥à¹Œ ZIP: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mself\u001b[39m.scaler_in = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mself\u001b[39m.scaler_out = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: âŒ à¹„à¸¡à¹ˆà¸žà¸šà¹„à¸Ÿà¸¥à¹Œ ZIP: D:\\Project_end\\New_world\\my_project\\config\\RC_Tank_Env_Training2_scalers.zip"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import joblib\n",
    "import yaml\n",
    "import io\n",
    "from pathlib import Path\n",
    "\n",
    "class ScalingZipLoader:\n",
    "    def __init__(self, zip_path):\n",
    "        self.zip_path = Path(zip_path)\n",
    "        if not self.zip_path.exists():\n",
    "            raise FileNotFoundError(f\"âŒ à¹„à¸¡à¹ˆà¸žà¸šà¹„à¸Ÿà¸¥à¹Œ ZIP: {zip_path}\")\n",
    "\n",
    "        self.scaler_in = None\n",
    "        self.scaler_out = None\n",
    "        self.metadata = None\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"à¹‚à¸«à¸¥à¸” input/output scaler à¹à¸¥à¸° metadata à¸ˆà¸²à¸ ZIP\"\"\"\n",
    "        with zipfile.ZipFile(self.zip_path, \"r\") as zipf:\n",
    "            # âœ… à¹‚à¸«à¸¥à¸” input_scaler.pkl\n",
    "            with zipf.open(\"input_scaler.pkl\") as f:\n",
    "                buffer = io.BytesIO(f.read())\n",
    "                self.scaler_in = joblib.load(buffer)\n",
    "\n",
    "            # âœ… à¹‚à¸«à¸¥à¸” output_scaler.pkl\n",
    "            with zipf.open(\"output_scaler.pkl\") as f:\n",
    "                buffer = io.BytesIO(f.read())\n",
    "                self.scaler_out = joblib.load(buffer)\n",
    "\n",
    "            # âœ… à¹‚à¸«à¸¥à¸” metadata.yaml\n",
    "            with zipf.open(\"metadata.yaml\") as f:\n",
    "                self.metadata = yaml.safe_load(f)\n",
    "\n",
    "        print(f\"ðŸ“¦ Loaded ZIP successfully: {self.zip_path}\")\n",
    "        return self.scaler_in, self.scaler_out, self.metadata\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ðŸ§ª à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¸à¸²à¸£à¹ƒà¸Šà¹‰à¸‡à¸²à¸™\n",
    "    zip_file = r\"D:\\Project_end\\New_world\\my_project\\config\\RC_Tank_Env_Training2_scalers.zip\"\n",
    "    loader = ScalingZipLoader(zip_file)\n",
    "    scaler_in, scaler_out, metadata = loader.load()\n",
    "\n",
    "    print(\"\\nðŸŽ¯ Metadata summary:\")\n",
    "    print(yaml.dump(metadata, allow_unicode=True, sort_keys=False))\n",
    "\n",
    "    # âœ… à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¸à¸²à¸£à¹ƒà¸Šà¹‰à¸‡à¸²à¸™ scaler\n",
    "    import numpy as np\n",
    "    sample_input = np.array([[0.5]])\n",
    "    scaled_input = scaler_in.transform(sample_input)\n",
    "    print(\"\\nSample input 0.5 scaled:\", scaled_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "905dae62",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\Project_end\\\\New_world\\\\my_project\\\\data\\\\raw\\\\pwm_duty_0.10_freq_0.10_pwm.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m     16\u001b[39m data_file = \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mD:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mProject_end\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mNew_world\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mmy_project\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mraw\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mpwm_duty_0.10_freq_0.10_pwm.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Load scalers (à¸ˆà¸²à¸ ZIP à¸à¹‡à¹„à¸”à¹‰)\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¹ƒà¸Šà¹‰ MinMaxScaler à¹ƒà¸«à¸¡à¹ˆ\u001b[39;00m\n\u001b[32m     23\u001b[39m input_features = [\u001b[33m\"\u001b[39m\u001b[33mDATA_INPUT\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDATA_OUTPUT\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project_end\\New_world\\NEW_WORLD_SETUP\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project_end\\New_world\\NEW_WORLD_SETUP\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project_end\\New_world\\NEW_WORLD_SETUP\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project_end\\New_world\\NEW_WORLD_SETUP\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project_end\\New_world\\NEW_WORLD_SETUP\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'D:\\\\Project_end\\\\New_world\\\\my_project\\\\data\\\\raw\\\\pwm_duty_0.10_freq_0.10_pwm.csv'"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# LSTM RC Tank Prediction Example\n",
    "# -----------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "# -----------------------------\n",
    "# Load dataset\n",
    "# -----------------------------\n",
    "data_file = r\"D:\\Project_end\\New_world\\my_project\\data\\raw\\pwm_duty_0.10_freq_0.10_pwm.csv\"\n",
    "df = pd.read_csv(data_file)\n",
    "\n",
    "# -----------------------------\n",
    "# Load scalers (à¸ˆà¸²à¸ ZIP à¸à¹‡à¹„à¸”à¹‰)\n",
    "# -----------------------------\n",
    "# à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¹ƒà¸Šà¹‰ MinMaxScaler à¹ƒà¸«à¸¡à¹ˆ\n",
    "input_features = [\"DATA_INPUT\", \"DATA_OUTPUT\"]\n",
    "output_features = [\"DATA_OUTPUT\"]\n",
    "\n",
    "scaler_in = MinMaxScaler()\n",
    "scaler_out = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler_in.fit_transform(df[input_features].values)\n",
    "y_scaled = scaler_out.fit_transform(df[output_features].values)\n",
    "\n",
    "# -----------------------------\n",
    "# à¸ªà¸£à¹‰à¸²à¸‡ sequences à¸ªà¸³à¸«à¸£à¸±à¸š LSTM\n",
    "# -----------------------------\n",
    "window_size = 30\n",
    "\n",
    "def create_sequences(X, y, window_size=30):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - window_size):\n",
    "        X_seq.append(X[i:i+window_size])\n",
    "        y_seq.append(y[i+window_size])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_scaled, window_size=window_size)\n",
    "\n",
    "# à¹à¸šà¹ˆà¸‡ train/val/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n",
    "\n",
    "# à¹à¸›à¸¥à¸‡à¹€à¸›à¹‡à¸™ torch.Tensor\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# -----------------------------\n",
    "# à¸ªà¸£à¹‰à¸²à¸‡ LSTM model\n",
    "# -----------------------------\n",
    "class LSTM_Predictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, output_size=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "input_size = X_train.shape[2]\n",
    "output_size = y_train.shape[1]\n",
    "\n",
    "model = LSTM_Predictor(input_size=input_size, hidden_size=64, num_layers=2, output_size=output_size)\n",
    "\n",
    "# -----------------------------\n",
    "# Training setup\n",
    "# -----------------------------\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 50\n",
    "batch_size = 64\n",
    "\n",
    "# -----------------------------\n",
    "# Training loop\n",
    "# -----------------------------\n",
    "for epoch in range(epochs):\n",
    "    permutation = torch.randperm(X_train.size(0))\n",
    "    epoch_loss = 0\n",
    "    for i in range(0, X_train.size(0), batch_size):\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_X, batch_y = X_train[indices], y_train[indices]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * batch_X.size(0)\n",
    "\n",
    "    epoch_loss /= X_train.size(0)\n",
    "    if (epoch+1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.6f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Test & Plot\n",
    "# -----------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test).numpy()\n",
    "\n",
    "# à¹à¸›à¸¥à¸‡à¸à¸¥à¸±à¸š scale à¹€à¸”à¸´à¸¡\n",
    "y_pred_orig = scaler_out.inverse_transform(y_pred)\n",
    "y_test_orig = scaler_out.inverse_transform(y_test.numpy())\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(y_test_orig, label=\"True Tank Level\")\n",
    "plt.plot(y_pred_orig, label=\"Predicted Tank Level\", alpha=0.7)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Sample\")\n",
    "plt.ylabel(\"Tank Level\")\n",
    "plt.title(\"LSTM Prediction RC Tank\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723fcb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Project root added to sys.path: D:\\Project_end\\New_world\\my_project\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# project_root = my_project\n",
    "project_root = Path().resolve().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "print(\"âœ… Project root added to sys.path:\", project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1225aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LSTMForecaster', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'nn', 'torch']\n"
     ]
    }
   ],
   "source": [
    "import src.models.lstm_model\n",
    "print(dir(src.models.lstm_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bef394e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "âœ… Successfully loaded artifacts from: Test_scale1_scalers.zip\n",
      "Epoch 001 | Train Loss: 143871.636856 | Val Loss: 137650.076420\n",
      "Epoch 002 | Train Loss: 126733.987378 | Val Loss: 121638.882805\n",
      "Epoch 003 | Train Loss: 111790.312463 | Val Loss: 107243.515578\n",
      "Epoch 004 | Train Loss: 98250.455600 | Val Loss: 94088.150048\n",
      "Epoch 005 | Train Loss: 85899.356970 | Val Loss: 82092.091552\n",
      "Epoch 006 | Train Loss: 74650.619076 | Val Loss: 71174.706417\n",
      "Epoch 007 | Train Loss: 64426.045043 | Val Loss: 61261.972175\n",
      "Epoch 008 | Train Loss: 55132.235712 | Val Loss: 52329.592772\n",
      "Epoch 009 | Train Loss: 46713.843441 | Val Loss: 44070.123397\n",
      "Epoch 010 | Train Loss: 39143.443633 | Val Loss: 36746.886781\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 82\u001b[39m\n\u001b[32m     80\u001b[39m X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE).unsqueeze(-\u001b[32m1\u001b[39m)\n\u001b[32m     81\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m y_pred = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m loss = criterion(y_pred, y_batch)\n\u001b[32m     84\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project_end\\New_world\\NEW_WORLD_SETUP\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project_end\\New_world\\NEW_WORLD_SETUP\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project_end\\New_world\\my_project\\src\\models\\lstm_model.py:107\u001b[39m, in \u001b[36mLSTM_MODEL.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    104\u001b[39m     out, \u001b[38;5;28mself\u001b[39m.hidden_state = \u001b[38;5;28mself\u001b[39m.lstm(x, \u001b[38;5;28mself\u001b[39m.hidden_state)\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    106\u001b[39m     \u001b[38;5;66;03m# Stateless forward\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     out, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[38;5;66;03m# Take output at the final time step and map to output dimension\u001b[39;00m\n\u001b[32m    110\u001b[39m out = \u001b[38;5;28mself\u001b[39m.fc(out[:, -\u001b[32m1\u001b[39m, :])\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project_end\\New_world\\NEW_WORLD_SETUP\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project_end\\New_world\\NEW_WORLD_SETUP\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project_end\\New_world\\NEW_WORLD_SETUP\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1124\u001b[39m, in \u001b[36mLSTM.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m   1121\u001b[39m         hx = \u001b[38;5;28mself\u001b[39m.permute_hidden(hx, sorted_indices)\n\u001b[32m   1123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1124\u001b[39m     result = \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1125\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1126\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1127\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1128\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1129\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1130\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1131\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1136\u001b[39m     result = _VF.lstm(\n\u001b[32m   1137\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1138\u001b[39m         batch_sizes,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1145\u001b[39m         \u001b[38;5;28mself\u001b[39m.bidirectional,\n\u001b[32m   1146\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "%run setup_path.py\n",
    "\n",
    "import torch\n",
    "from src.data.scaling_loader import ScalingZipLoader\n",
    "from src.data.sequence_builder import create_sequences\n",
    "from src.models.lstm_model import LSTM_MODEL\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from src.utils.logger import Logger\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# 1ï¸âƒ£ Configuration\n",
    "# ===========================\n",
    "DATA_FILE = Path(r\"D:\\Project_end\\New_world\\my_project\\data\\raw\\pwm_duty_0.50_freq_0.01_pwm.csv\")\n",
    "SCALER_ZIP = Path(r\"D:\\Project_end\\New_world\\my_project\\config\\Test_scale1_scalers.zip\")\n",
    "WINDOW_SIZE = 10\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 1e-3\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(DEVICE)\n",
    "# ===========================\n",
    "# 2ï¸âƒ£ Load data\n",
    "# ===========================\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "X_raw = df[[\"DATA_INPUT\"]].values\n",
    "y_raw = df[[\"DATA_OUTPUT\"]].values\n",
    "\n",
    "# ===========================\n",
    "# 3ï¸âƒ£ Load scalers\n",
    "# ===========================\n",
    "scaler_loader = ScalingZipLoader(SCALER_ZIP)\n",
    "X_scaled = scaler_loader.transform_input(X_raw)\n",
    "y_scaled = scaler_loader.inverse_output(y_raw)  # à¸«à¸£à¸·à¸­ transform_output à¸–à¹‰à¸²à¸¡à¸µ\n",
    "\n",
    "# ===========================\n",
    "# 4ï¸âƒ£ Create sequences\n",
    "# ===========================\n",
    "X_seq, y_seq = create_sequences(np.column_stack((X_scaled, y_scaled)), window_size=WINDOW_SIZE)\n",
    "\n",
    "# ===========================\n",
    "# 5ï¸âƒ£ Train/Val split\n",
    "# ===========================\n",
    "dataset = TensorDataset(torch.tensor(X_seq, dtype=torch.float32),\n",
    "                        torch.tensor(y_seq, dtype=torch.float32))\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# ===========================\n",
    "# 6ï¸âƒ£ Model & Optimizer\n",
    "# ===========================\n",
    "input_dim = X_seq.shape[2]\n",
    "model = LSTM_MODEL(input_dim=input_dim, hidden_dim=64, output_dim=1, stateful=False).to(DEVICE)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# ===========================\n",
    "# 7ï¸âƒ£ Logger\n",
    "# ===========================\n",
    "logger = Logger()\n",
    "\n",
    "# ===========================\n",
    "# 8ï¸âƒ£ Training Loop\n",
    "# ===========================\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE).unsqueeze(-1)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * X_batch.size(0)\n",
    "    \n",
    "    epoch_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE).unsqueeze(-1)\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            val_loss += loss.item() * X_batch.size(0)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    # Log\n",
    "    logger.add_data_log([\"epoch\", \"train_loss\", \"val_loss\"], [[epoch], [epoch_loss], [val_loss]])\n",
    "    print(f\"Epoch {epoch:03d} | Train Loss: {epoch_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "# ===========================\n",
    "# 9ï¸âƒ£ Save trained model\n",
    "# ===========================\n",
    "MODEL_PATH = Path(\"D:/Project_end/New_world/my_project/models/lstm_model.pth\")\n",
    "MODEL_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "torch.save(model.state_dict(), MODEL_PATH)\n",
    "print(f\"âœ… Model saved at {MODEL_PATH}\")\n",
    "\n",
    "# ===========================\n",
    "# 10ï¸âƒ£ Save training log\n",
    "# ===========================\n",
    "logger.save_to_csv(\"training_log\", folder_name=\"logs\")\n",
    "print(\"âœ… Training log saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3241a440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_chunksize': 51, 'recommended_range': (150, 500)}\n"
     ]
    }
   ],
   "source": [
    "def suggest_chunksize(seq_len, multiplier_range=(3, 10)):\n",
    "    min_chunksize = seq_len + 1\n",
    "    recommended_min = seq_len * multiplier_range[0]\n",
    "    recommended_max = seq_len * multiplier_range[1]\n",
    "    \n",
    "    return {\n",
    "        'min_chunksize': min_chunksize,\n",
    "        'recommended_range': (recommended_min, recommended_max)\n",
    "    }\n",
    "\n",
    "seq_len = 50\n",
    "chunksize_info = suggest_chunksize(seq_len)\n",
    "print(chunksize_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef13e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Successfully loaded artifacts from: Test_scale1_scalers.zip\n",
      "INPUT SCALE --> MIN: [0.0] | MAX: [24.0]\n",
      "OUTPUT SCALE --> MIN: [1.389728099221949e-06] | MAX: [23.9999986102719]\n",
      "âœ… Successfully loaded artifacts from: Test_scale1_scalers.zip\n",
      "X shape:(11, 1) | y shape:(1,) | dataset:39560 row\n",
      "[0.30969242]\n"
     ]
    }
   ],
   "source": [
    "%run setup_path.py\n",
    "from src.data.scaling_loader import ScalingZipLoader\n",
    "from src.data.sequence_builder import create_sequences\n",
    "from src.models.lstm_model import LSTM_MODEL\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from src.utils.logger import Logger\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "\n",
    "# LOAD SCALE\n",
    "PATH_FILE_SCALE = r\"D:\\Project_end\\New_world\\my_project\\config\\Test_scale1_scalers.zip\"\n",
    "Path_FILE_DATA = Path(r\"D:\\Project_end\\New_world\\my_project\\data\\raw\")\n",
    "scale_loader = ScalingZipLoader(Path(PATH_FILE_SCALE))\n",
    "#scale_loader.summary()\n",
    "print(f\"INPUT SCALE --> MIN: {scale_loader.scaler_in.data_min_.tolist()} | MAX: {scale_loader.scaler_in.data_max_.tolist()}\")\n",
    "print(f\"OUTPUT SCALE --> MIN: {scale_loader.scaler_out.data_min_.tolist()} | MAX: {scale_loader.scaler_out.data_max_.tolist()}\")\n",
    "csv_files = sorted(list(Path_FILE_DATA.glob(\"*.csv\")))\n",
    "\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "\n",
    "    def __init__(self, folder_path, scale_path, sequence_size,\n",
    "                 input_col='DATA_INPUT', output_col='DATA_OUTPUT', \n",
    "                 file_ext='.csv', chunksize=1000,\n",
    "                 allow_padding=True, pad_value=0.0):\n",
    "        \n",
    "\n",
    "        self.folder_path = Path(folder_path)\n",
    "        self.scale_path = Path(scale_path)\n",
    "        self.sequence_size = sequence_size\n",
    "        self.input_col = input_col\n",
    "        self.output_col = output_col\n",
    "        self.file_ext = file_ext\n",
    "        self.chunksize = chunksize\n",
    "        self.allow_padding = allow_padding\n",
    "        self.pad_value = pad_value\n",
    "\n",
    "        \n",
    "        self.scaling_loader = ScalingZipLoader(self.scale_path)\n",
    "        self.scaler_input = self.scaling_loader.scaler_in  \n",
    "        self.scaler_output = self.scaling_loader.scaler_out\n",
    "\n",
    "        \n",
    "        self.X, self.y = self.load_and_create_sequences(folder_path, file_ext)\n",
    "\n",
    "    def load_and_create_sequences(self, folder_path, file_ext):\n",
    "        X_all, y_all = [], []\n",
    "        files = [f for f in os.listdir(folder_path) if f.endswith(file_ext)]\n",
    "\n",
    "        for file_name in files:\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "            for chunk in pd.read_csv(file_path, chunksize=self.chunksize):\n",
    "                # scale input/output\n",
    "                inputs = self.scaler_input.transform(chunk[self.input_col].values)\n",
    "                outputs = self.scaler_output.transform(chunk[self.output_col].values)\n",
    "\n",
    "                X_seq, y_seq = self.create_sequences(inputs, outputs)\n",
    "\n",
    "                if len(X_seq) > 0:\n",
    "                    X_all.append(X_seq)\n",
    "                    y_all.append(y_seq)\n",
    "\n",
    "        return np.vstack(X_all), np.vstack(y_all)\n",
    "\n",
    "\n",
    "    def create_sequences(self, X_data, y_data):\n",
    "        Xs, ys = [], []\n",
    "        data_len = len(X_data)\n",
    "        \n",
    "        if data_len < self.sequence_size and self.allow_padding:\n",
    "            # pad à¹à¸¥à¹‰à¸§à¸ªà¸£à¹‰à¸²à¸‡ sequence à¹€à¸”à¸µà¸¢à¸§\n",
    "            X_pad = self.pad_or_truncate(X_data)\n",
    "            y_pad = self.pad_or_truncate(y_data)\n",
    "\n",
    "            return np.array([X_pad]), np.array([y_pad[-1]])\n",
    "        \n",
    "        for i in range(max(0, data_len - self.sequence_size)): \n",
    "            Xs.append(X_data[i:i + self.sequence_size])\n",
    "            ys.append(y_data[i + self.sequence_size])\n",
    "\n",
    "        return np.array(Xs), np.array(ys)\n",
    "    \n",
    "    def pad_or_truncate(self, seq):\n",
    "        if len(seq) < self.sequence_size:\n",
    "            pad_size = self.sequence_size - len(seq)\n",
    "            pad = np.full((pad_size, seq.shape[1]), self.pad_value)\n",
    "            seq = np.vstack((pad, seq))\n",
    "        elif len(seq) > self.sequence_size:\n",
    "            seq = seq[-self.sequence_size:]\n",
    "        return seq\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "\n",
    "FOLDER_FILE = r\"D:\\Project_end\\New_world\\my_project\\data\\raw\"\n",
    "PATH_SCALE = r\"D:\\Project_end\\New_world\\my_project\\config\\Test_scale1_scalers.zip\"\n",
    "\n",
    "dataset = SequenceDataset(\n",
    "    folder_path=FOLDER_FILE,\n",
    "    scale_path=PATH_SCALE,\n",
    "    sequence_size=11,\n",
    "    input_col=['DATA_INPUT'],\n",
    "    output_col=[\"DATA_OUTPUT\"],\n",
    "    chunksize=1000,\n",
    "    allow_padding=True, pad_value=0.0\n",
    ")\n",
    "\n",
    "X, y = dataset[0]\n",
    "print(f\"X shape:{X.shape} | y shape:{y.shape} | dataset:{len(dataset)} row\")\n",
    "print(y)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NEW_WORLD_SETUP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
