import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import numpy as np
import torch
import os
from pathlib import Path

# =============================================================================
# Section 1: The "Gold Standard" Scaler Fitting
# (‡∏ó‡∏≥‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤)
# =============================================================================
def create_global_scaler(folder_path, chunk_size=10000):
    """
    ‡∏ß‡∏ô‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå CSV ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡∏•‡∏∞‡∏™‡πà‡∏ß‡∏ô (chunk) ‡πÄ‡∏û‡∏∑‡πà‡∏≠ fit scaler
    ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ RAM ‡πÄ‡∏¢‡∏≠‡∏∞‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ partial_fit
    
    Args:
        folder_path (str): ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ü‡∏•‡πå CSV
        chunk_size (int): ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏≠‡πà‡∏≤‡∏ô‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏£‡∏±‡πâ‡∏á
        
    Returns:
        tuple: (scaler_input, scaler_output) ‡∏ó‡∏µ‡πà fit ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß
    """
    print("--- üöÄ Starting Global Scaler Fitting Process ---")
    
    scaler_input = MinMaxScaler()
    scaler_output = MinMaxScaler()
    
    # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå .csv ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå
    csv_files = list(Path(folder_path).glob("*.csv"))
    if not csv_files:
        print(f"‚ö†Ô∏è No CSV files found in '{folder_path}'")
        return None, None
        
    print(f"Found {len(csv_files)} files to process.")

    # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡πÑ‡∏õ‡∏ó‡∏µ‡∏•‡∏∞‡πÑ‡∏ü‡∏•‡πå
    for i, file_path in enumerate(csv_files):
        print(f"  -> Processing file {i+1}/{len(csv_files)}: {file_path.name}")
        
        # ‡πÉ‡∏ä‡πâ pd.read_csv ‡πÅ‡∏ö‡∏ö iterator ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡πà‡∏≤‡∏ô‡∏ó‡∏µ‡∏•‡∏∞ chunk
        with pd.read_csv(file_path, chunksize=chunk_size) as reader:
            for chunk in reader:
                # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏™‡∏ô‡πÉ‡∏à
                input_chunk = chunk[['DATA_INPUT']].values
                output_chunk = chunk[['DATA_OUTPUT']].values
                
                # *** ‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏Ç‡∏≠‡∏á‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏ô‡∏µ‡πâ ***
                scaler_input.partial_fit(input_chunk)
                scaler_output.partial_fit(output_chunk)

    print("\n--- ‚úÖ Global Scaler Fitting Complete! ---")
    print(f"Input Scaler Range: {scaler_input.data_min_[0]:.2f} to {scaler_input.data_max_[0]:.2f}")
    print(f"Output Scaler Range: {scaler_output.data_min_[0]:.2f} to {scaler_output.data_max_[0]:.2f}")
    
    return scaler_input, scaler_output

# --- ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ä‡∏∑‡πà‡∏≠ 'raw' ---
DATA_FOLDER = "raw" 
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÅ‡∏•‡∏∞‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≥‡∏•‡∏≠‡∏á (‡∏´‡∏≤‡∏Å‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ)
if not os.path.exists(DATA_FOLDER):
    os.makedirs(DATA_FOLDER)
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≥‡∏•‡∏≠‡∏á 3 ‡πÑ‡∏ü‡∏•‡πå
    for i in range(3):
        n_points = 2000
        input_data = np.random.rand(n_points, 1) * (i + 1) * 10
        output_data = np.sin(np.arange(n_points) / 50) * (i + 1) * 20 + 30
        df = pd.DataFrame({'DATA_INPUT': input_data.flatten(), 'DATA_OUTPUT': output_data.flatten()})
        df.to_csv(f"{DATA_FOLDER}/data_log_sim_{i+1}.csv", index=False)

# **‡∏£‡∏±‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á Scaler ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤**
global_scaler_input, global_scaler_output = create_global_scaler(DATA_FOLDER)


# =============================================================================
# Section 2: Using the Pre-fitted Scaler in a Memory-Efficient Generator
# (‡∏ô‡∏≥ Scaler ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å Section 1 ‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏ï‡∏≠‡∏ô‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•)
# =============================================================================
def data_generator_with_global_scaler(file_list, scaler_in, scaler_out, window_size):
    """
    Generator ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ Scaler ‡∏ã‡∏∂‡πà‡∏á‡∏ñ‡∏π‡∏Å fit ‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
    """
    for file_path in file_list:
        df = pd.read_csv(file_path)
        data = df[['DATA_INPUT', 'DATA_OUTPUT']].values.astype(np.float32)
        
        # *** ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£ .fit() ‡∏≠‡∏µ‡∏Å‡∏ï‡πà‡∏≠‡πÑ‡∏õ ‡πÉ‡∏ä‡πâ .transform() ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ***
        scaled_input = scaler_in.transform(data[:, 0].reshape(-1, 1))
        scaled_output = scaler_out.transform(data[:, 1].reshape(-1, 1))
        scaled_data = np.hstack([scaled_input, scaled_output])
        
        X, y = create_sequences(scaled_data, window_size) # (‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏≠‡∏¢‡∏π‡πà)
        
        yield torch.from_numpy(X).float(), torch.from_numpy(y).float().view(-1, 1)

# --- ‡∏ï‡∏≠‡∏ô‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ---
if global_scaler_input:
    file_paths = list(Path(DATA_FOLDER).glob("*.csv"))
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á generator ‡∏ó‡∏µ‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
    my_generator = data_generator_with_global_scaler(
        file_list=file_paths,
        scaler_in=global_scaler_input,
        scaler_out=global_scaler_output,
        window_size=30
    )
    
    # ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ô‡∏≥ my_generator ‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô training loop ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢
    print("\nGenerator is ready to be used for training with the global scaler.")
    # for X_batch, y_batch in my_generator:
    #     # ... training logic ...
    #     pass